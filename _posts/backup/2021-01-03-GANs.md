---
layout: post
title: Generative Adversarial Networks (GANs)
github: https://github.com/BenoitLeguay/GAN_IconClass
---

Curious about GANs framework, I wanted to deal with it in depth. I completed the online course ([available on Coursera](https://www.coursera.org/specializations/generative-adversarial-networks-gans)), and then I decided to implement most of framework versions (vanilla DCGANs, WGANs, Conditional GANs etc...)

GANs are powerful architecture that can learn to generate data with same statistics of a given set. They have many applications: generate unreal human faces based on true ones, up-scale image or video resolution, create DNA sequences with certain properties,  etc... 

<br/>

The main idea behind GAN is to use a *Generator* that creates fake examples and a *Discriminator* that tries to find these fake examples among the reals. Through an iterative process, it's likely that the generator creates more accurate example, and the discriminator classifies better fake and reals examples. Both the *Discriminator* and the *Generator* are neural networks. This duel between our 2 networks involves a *Game Theory* component, that we'll discuss later on. 



We will start with a very basic example, using a simple GAN architecture and dataset.

<br/>

<br/>

### 1) Vanilla GAN

<br/>

**Dataset**

Let's start with the dataset, I generate a population from a Gaussian Multivariate distribution with random mean and covariance. For visualization purpose, I make this example with 2-dimensional data. 

![gans-real.png]({{site.baseurl}}/images/gans/gans-real-2d.png)

*Real examples (2-dimensional)*

About the architecture, we've got 2 fully connected neural networks with 2 hidden layers. 

<br/>

**GANs architecture**

<br/>

*Loss*

In GANs we use the *minimax loss*. Described mathematically, the discriminator seeks to maximize the  average of the log probability for real images and the log of the  inverted probabilities of fake images. The generator seeks to minimize the log of the inverse probability predicted by the discriminator for fake images.

$$E_x[log(D(x))] + E_z[log(1 - D(G(z)))]$$

<br/>

*Generator*

The generator takes a random vector as input of size *z* and outputs a vector having the same size as $$x_i$$. 

<br/>

*Discriminator*

The discriminator takes a input shape vector (real and fake) and output a classification score. 

<br/>

Thanks to this example, we can easily observe and compare our real and fake data. Here below we have the evolution of the fake generated during the training process:

![gans-fake-real.gif]({{site.baseurl}}/images/gans/gans-fake-real.gif)

*Generated fake along with real examples regarding epochs*

<br/>

GANs learn the distribution statistics and can output almost real example with a quit good variety. Obviously the goal is to produce diversified example and to prevent mode collapse.  

A **mode collapse** refers to a generator model that is only capable of generating one or a small subset of different outcomes, or modes. The output resembles a real one, the discriminator is fooled, but the Generator can only imitate this little area of the distribution. 

![gans-mode_collapse.png]({{site.baseurl}}/images/gans/gans-mode_collapse.png)

*Example of a Generator suffering from mode collapse*

<br/><br/>

An interesting parameter to play with in this example is the dimension of the Z input. Z is the noise vector, a random normal generated vector that is the *Generator's* input. The bigger the dimension of Z, the more information is contained in it and thus easier it is for the *Generator* to achieve good diversity. Let's compare 2 training process with Z having different lengths.  

![gans-zdim-1.gif]({{site.baseurl}}/images/gans/gans-zdim-1.gif)

*Z dimension: 1*

![gans-zdim-100.gif]({{site.baseurl}}/images/gans/gans-zdim-100.gif)

*Z dimension: 100*

On one side, the generated population lacks diversity, GANs do well inferring the overall trend of the data but is unable to capture the variance on another dimension. On the other side, our GANs reach a better variety but are more biased toward the training set.

<br/><br/>

**Games Theory**

We will not find explicit games theory components in GANs, but there is theoretical aspect in it. We can see a GANs as a zero-sum (*i.e minimax*) non-cooperative game. The minimax loss equations sums it up clearly, when one wins the other loses. From a game theory point of view, we reach convergence when both opposing side find a Nash Equilibrium. That is a concept where the optimal outcome of a game is one where no player has an  incentive to deviate from their chosen strategy after considering an opponent's choice. 

In GANs, your opponent always countermeasures your strategy, this makes the models having a hard time converging. Thus, we can easily demonstrate that gradient descent in a minimax game has no convergence guarantee. 

# 2) DCGANs, WGANs, ACGANs

You can find detailed explanations, architecture comparison and hyper parameters analysis [here]({% post_url 2021-05-02-GANs-Train-Pokemon %}).

<br/>